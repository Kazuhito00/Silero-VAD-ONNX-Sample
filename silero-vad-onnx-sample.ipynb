{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# パッケージインストール"
      ],
      "metadata": {
        "id": "MwXZtCzbajEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q onnx onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VH4G9YJw9-s_",
        "outputId": "1e9597d1-c04f-478c-fa15-69b43c34bc06"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 重み、サンプル音声ダウンロード"
      ],
      "metadata": {
        "id": "tEi6qg5uafB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/snakers4/silero-vad/raw/refs/heads/master/src/silero_vad/data/silero_vad.onnx -q\n",
        "!wget https://models.silero.ai/vad_models/en.wav -O en_example.wav -q"
      ],
      "metadata": {
        "id": "V5N_RcRY640n"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ONNX取り扱い用クラス、その他ユーティリティ"
      ],
      "metadata": {
        "id": "Dtv8juN3ao8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import numpy as np\n",
        "import onnxruntime\n",
        "\n",
        "class OnnxWrapper:\n",
        "    def __init__(self, path, force_onnx_cpu=False):\n",
        "        opts = onnxruntime.SessionOptions()\n",
        "        opts.inter_op_num_threads = 1\n",
        "        opts.intra_op_num_threads = 1\n",
        "\n",
        "        if force_onnx_cpu and 'CPUExecutionProvider' in onnxruntime.get_available_providers():\n",
        "            self.session = onnxruntime.InferenceSession(path, providers=['CPUExecutionProvider'], sess_options=opts)\n",
        "        else:\n",
        "            self.session = onnxruntime.InferenceSession(path, sess_options=opts)\n",
        "\n",
        "        self.reset_states()\n",
        "        if '16k' in path:\n",
        "            warnings.warn('This model supports only 16000 sampling rate!')\n",
        "            self.sample_rates = [16000]\n",
        "        else:\n",
        "            self.sample_rates = [8000, 16000]\n",
        "\n",
        "    def _validate_input(self, x: np.ndarray, sr: int) -> tuple[np.ndarray, int]:\n",
        "        if x.ndim == 1:\n",
        "            x = np.expand_dims(x, axis=0)\n",
        "        if x.ndim > 2:\n",
        "            raise ValueError(f\"Too many dimensions for input audio chunk: {x.ndim}\")\n",
        "\n",
        "        if sr != 16000 and (sr % 16000 == 0):\n",
        "            step = sr // 16000\n",
        "            x = x[:, ::step]\n",
        "            sr = 16000\n",
        "\n",
        "        if sr not in self.sample_rates:\n",
        "            raise ValueError(f\"Supported sampling rates: {self.sample_rates} (or multiple of 16000)\")\n",
        "        if sr / x.shape[1] > 31.25:\n",
        "            raise ValueError(\"Input audio chunk is too short\")\n",
        "\n",
        "        return x, sr\n",
        "\n",
        "    def reset_states(self, batch_size: int = 1):\n",
        "        self._state = np.zeros((2, batch_size, 128), dtype=np.float32)\n",
        "        self._context = None\n",
        "        self._last_sr = 0\n",
        "        self._last_batch_size = 0\n",
        "\n",
        "    def __call__(self, x: np.ndarray, sr: int) -> np.ndarray:\n",
        "        x, sr = self._validate_input(x, sr)\n",
        "        num_samples = 512 if sr == 16000 else 256\n",
        "\n",
        "        if x.shape[-1] != num_samples:\n",
        "            raise ValueError(f\"Provided number of samples is {x.shape[-1]} (Supported: 256 for 8kHz, 512 for 16kHz)\")\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        context_size = 64 if sr == 16000 else 32\n",
        "\n",
        "        if self._last_sr != sr or self._last_batch_size != batch_size or self._last_batch_size == 0:\n",
        "            self.reset_states(batch_size)\n",
        "\n",
        "        if self._context is None:\n",
        "            self._context = np.zeros((batch_size, context_size), dtype=np.float32)\n",
        "\n",
        "        x = np.concatenate([self._context, x], axis=1)\n",
        "\n",
        "        ort_inputs = {\n",
        "            'input': x.astype(np.float32),\n",
        "            'state': self._state.astype(np.float32),\n",
        "            'sr': np.array(sr, dtype=np.int64)\n",
        "        }\n",
        "\n",
        "        out, new_state = self.session.run(None, ort_inputs)\n",
        "        self._state = new_state\n",
        "        self._context = x[:, -context_size:]\n",
        "        self._last_sr = sr\n",
        "        self._last_batch_size = batch_size\n",
        "\n",
        "        return out\n",
        "\n",
        "    def audio_forward(self, x: np.ndarray, sr: int) -> np.ndarray:\n",
        "        outs = []\n",
        "        x, sr = self._validate_input(x, sr)\n",
        "        self.reset_states()\n",
        "        num_samples = 512 if sr == 16000 else 256\n",
        "\n",
        "        if x.shape[1] % num_samples != 0:\n",
        "            pad_num = num_samples - (x.shape[1] % num_samples)\n",
        "            x = np.pad(x, ((0, 0), (0, pad_num)), mode='constant')\n",
        "\n",
        "        for i in range(0, x.shape[1], num_samples):\n",
        "            wavs_batch = x[:, i:i + num_samples]\n",
        "            out_chunk = self.__call__(wavs_batch, sr)\n",
        "            outs.append(out_chunk)\n",
        "\n",
        "        return np.concatenate(outs, axis=1)"
      ],
      "metadata": {
        "id": "oAkqlcJf64xz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import Callable, List\n",
        "\n",
        "class VADIterator:\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 threshold: float = 0.5,\n",
        "                 sampling_rate: int = 16000,\n",
        "                 min_silence_duration_ms: int = 100,\n",
        "                 speech_pad_ms: int = 30):\n",
        "        self.model = model\n",
        "        self.threshold = threshold\n",
        "        self.sampling_rate = sampling_rate\n",
        "\n",
        "        if sampling_rate not in [8000, 16000]:\n",
        "            raise ValueError('VADIterator supports only 8000 and 16000 Hz')\n",
        "\n",
        "        self.min_silence_samples = int(sampling_rate * min_silence_duration_ms / 1000)\n",
        "        self.speech_pad_samples = int(sampling_rate * speech_pad_ms / 1000)\n",
        "        self.reset_states()\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.model.reset_states()\n",
        "        self.triggered = False\n",
        "        self.temp_end = 0\n",
        "        self.current_sample = 0\n",
        "\n",
        "    def __call__(self, x: np.ndarray, return_seconds=False, time_resolution: int = 1):\n",
        "        \"\"\"\n",
        "        x: np.ndarray, shape [1, N] or [N]\n",
        "            Audio chunk\n",
        "        return_seconds: bool\n",
        "            If True, output start/end in seconds instead of samples\n",
        "        time_resolution: int\n",
        "            Decimal places for seconds\n",
        "        \"\"\"\n",
        "        if not isinstance(x, np.ndarray):\n",
        "            raise TypeError(\"Input audio must be a NumPy array\")\n",
        "\n",
        "        if x.ndim == 1:\n",
        "            x = np.expand_dims(x, axis=0)\n",
        "        elif x.ndim != 2:\n",
        "            raise ValueError(\"Input must be 1D or 2D NumPy array\")\n",
        "\n",
        "        window_size_samples = x.shape[1]\n",
        "        self.current_sample += window_size_samples\n",
        "\n",
        "        speech_prob = self.model(x, self.sampling_rate).item()\n",
        "\n",
        "        if speech_prob >= self.threshold and self.temp_end:\n",
        "            self.temp_end = 0\n",
        "\n",
        "        if speech_prob >= self.threshold and not self.triggered:\n",
        "            self.triggered = True\n",
        "            speech_start = max(0, self.current_sample - self.speech_pad_samples - window_size_samples)\n",
        "            return {'start': int(speech_start) if not return_seconds else round(speech_start / self.sampling_rate, time_resolution)}\n",
        "\n",
        "        if speech_prob < self.threshold - 0.15 and self.triggered:\n",
        "            if not self.temp_end:\n",
        "                self.temp_end = self.current_sample\n",
        "            if self.current_sample - self.temp_end < self.min_silence_samples:\n",
        "                return None\n",
        "            else:\n",
        "                speech_end = self.temp_end + self.speech_pad_samples - window_size_samples\n",
        "                self.temp_end = 0\n",
        "                self.triggered = False\n",
        "                return {'end': int(speech_end) if not return_seconds else round(speech_end / self.sampling_rate, time_resolution)}\n",
        "\n",
        "        return None"
      ],
      "metadata": {
        "id": "HZXgeb5f_e2S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# モデル読み込み"
      ],
      "metadata": {
        "id": "-JIIAexvavQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = OnnxWrapper(\"silero_vad.onnx\", force_onnx_cpu=True)"
      ],
      "metadata": {
        "id": "R21rY4kv_LCx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VAD"
      ],
      "metadata": {
        "id": "hiE_cNWObo6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "from scipy.signal import resample\n",
        "\n",
        "# サンプル音声読み込み\n",
        "wav, sr = sf.read(\"en_example.wav\")\n",
        "if wav.ndim > 1:\n",
        "    wav = np.mean(wav, axis=1)\n",
        "wav = wav.astype(np.float32)\n",
        "if sr != 16000:\n",
        "    num_samples = int(len(wav) * 16000 / sr)\n",
        "    wav = resample(wav, num_samples)"
      ],
      "metadata": {
        "id": "ZTLkQBNKa4QZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vad_iterator = VADIterator(model, sampling_rate=16000)\n",
        "window_size_samples = 512\n",
        "\n",
        "# VAD\n",
        "for i in range(0, len(wav), window_size_samples):\n",
        "    chunk = wav[i: i+ window_size_samples]\n",
        "    if len(chunk) < window_size_samples:\n",
        "        break\n",
        "    speech_dict = vad_iterator(chunk, return_seconds=True)\n",
        "    if speech_dict:\n",
        "        print(speech_dict, end=' ')\n",
        "vad_iterator.reset_states()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TQNHInU64qS",
        "outputId": "a762c2b9-42ab-46ca-e955-3cc03b2cd2c5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'start': 0.0} {'end': 2.1} {'start': 2.7} {'end': 4.9} {'start': 5.0} {'end': 6.8} {'start': 9.3} {'end': 13.4} {'start': 13.5} {'end': 15.2} {'start': 15.3} {'end': 15.8} {'start': 16.3} {'end': 17.9} {'start': 18.4} {'end': 19.6} {'start': 20.3} {'end': 37.6} {'start': 38.0} {'end': 38.9} {'start': 39.9} {'end': 43.3} {'start': 43.6} {'end': 44.6} {'start': 45.0} {'end': 46.8} {'start': 48.8} {'end': 50.0} {'start': 51.1} {'end': 53.4} {'start': 53.5} "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}